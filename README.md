# ğŸš€ DevOptiX â€“ DevOps Productivity Analyzer

DevOptiX is a modular and intelligent system for analyzing software development and deployment workflows to identify productivity bottlenecks and suggest data-driven improvements. It supports core DevOps and DORA metrics, detects bottlenecks, recommends improvements, visualizes trends, and integrates DORA metrics for high-performance insights. It is built to be extended with real-time integrations and ML-powered insights.

---

## ğŸ“Œ Features

- ğŸ” Workflow bottleneck detection (PR reviews, builds, deployments, etc.)
- ğŸ“ˆ DORA Metrics computation:
  - Deployment Frequency
  - Lead Time for Changes
  - Change Failure Rate (simulated)
  - Mean Time to Recovery (simulated)
- ğŸ¤– Recommendation Engine for team/process improvements
- ğŸ“Š Visual analytics (histograms, bottleneck trends)
- ğŸ§ª Synthetic data generation for demo/testing
- ğŸ“¦ Modular Python architecture
- âœ… Ready for real-time tool integration (GitHub, Jenkins, etc.)
- ğŸ–¥ï¸ Streamlit UI (planned)

---

## ğŸ“ Folder Structure

```
DevOptiX/
â”‚
â”œâ”€â”€ generate_data.py # Synthetic task generator
â”œâ”€â”€ compute_metrics.py # Core metrics computation
â”œâ”€â”€ bottleneck_detection.py # Detects process bottlenecks
â”œâ”€â”€ recommendation_engine.py # Task + DORA-based recommendations
â”œâ”€â”€ trend_analysis.py # Trend regression for time-based insights
â”œâ”€â”€ ml_anomaly_detector.py # Machine learning-based anomaly detection
â”œâ”€â”€ visualize.py # Multiple plots and visual analytics
â”œâ”€â”€ export.py # Exports data to CSV/JSON/TXT
â”œâ”€â”€ main.py # Entry point for the full pipeline
â””â”€â”€ outputs/ # All generated metrics, plots, and insights
```

---

## âš™ï¸ Installation & Setup

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/DevOptiX.git
cd DevOptiX
```

### 2. Create virtual environment (optional but recommended)

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Running the tool
```bash
python main.py
```

---

## ğŸ“Š Output Artifacts

All outputs are saved in the `outputs/` directory:

### ğŸ”¢ Metrics
- `metrics.csv`: All computed metrics per task  
- `dora_metrics.txt`: Overall DORA metrics summary  

### ğŸ“¦ Bottlenecks & Recommendations
- `bottlenecks.json`: Tasks with bottleneck stages  
- `task_recommendations.json` / `.txt`: Optimization suggestions  
- `dora_recommendations.json` / `.txt`: DORA-based team guidance  

### ğŸ§  Analysis & Trends
- `trend_regressions.json`: Stage trends over time  
- `anomalies.json`: Detected anomalies in performance  

### ğŸ“ˆ Visual Reports
- `pr_review_time.png`: PR review time distribution  
- `bottleneck_counts.png`: Bottleneck frequency by stage  
- `dora_metrics.png`: Bar chart of DORA metrics  
- `bottlenecks_by_stage_and_team.png`: Heatmap of delays by team/stage  
- `avg_pr_review_time_by_team.png`: Average PR review time per team  
- `lead_time_trend.png`: Sprint-based lead time changes  
- `developer_stage_heatmap.png`: Developer-stage bottleneck heatmap  

---

### ğŸ“Œ Sample Output

```yaml
ğŸ”§ Generating synthetic data...
ğŸ“Š Computing metrics...
ğŸ” Detecting bottlenecks...
ğŸ’¡ Generating task-based recommendations...
ğŸ“ˆ Computing DORA metrics...
ğŸ’¡ Generating DORA-based recommendations...
ğŸ“‰ Running trend analysis...
ğŸ¤– Running anomaly detection...
ğŸ“¤ Exporting outputs...
[EXPORT] Metrics exported to outputs\metrics.csv
[EXPORT] JSON data exported to outputs\bottlenecks.json
[EXPORT] JSON data exported to outputs\task_recommendations.json
[EXPORT] JSON data exported to outputs\dora_recommendations.json
[EXPORT] JSON data exported to outputs\trend_regressions.json
[EXPORT] JSON data exported to outputs\anomalies.json
ğŸ“Š Plotting insights...
[VISUAL] Saved plot to pr_review_time.png
[VISUAL] Saved bottleneck plot to outputs\bottleneck_counts.png
[VISUAL] Saved DORA metrics plot to outputs\dora_metrics.png
[VISUAL] Saved plot to outputs\bottlenecks_by_stage_and_team.png
[VISUAL] Saved avg stage durations to outputs\avg_pr_review_time_by_team.png
[VISUAL] Saved DORA trends to outputs\lead_time_trend.png
[VISUAL] Saved heatmap to outputs\developer_stage_heatmap.png
âœ… Done. Check the 'outputs/' folder for results.

ğŸ“ˆ DORA Metrics Summary:
   deployment_frequency_per_day: 8.33
   average_lead_time_hours: 29.0
   change_failure_rate_percent: 23.0
   mean_time_to_restore_hours: 1.59

```

---

## ğŸ› ï¸ Customization

You can configure or extend:

- **Team structure and number of developers**  
  Edit: `generate_synthetic_tasks()`

- **Stages to track** (e.g., add QA or staging phases)

- **Anomaly logic**  
  Edit: `ml_anomaly_detector.py`

- **Trend depth and sprint granularity**  
  Edit: `trend_analysis.py`

---

## ğŸ§­ Roadmap

- [ ] Real-time ingestion support (from CI/CD logs, GitHub APIs)  
- [ ] Web dashboard with interactive visualizations  
- [ ] Persistent database support for longitudinal studies  
- [ ] Integration with JIRA/GitHub metrics APIs  
- [ ] Role-based recommendations (Dev vs Ops vs Manager)

---

## ğŸ‘©â€ğŸ’» Contributing

Contributions are welcome!  
Feel free to open issues or submit PRs for enhancements, bug fixes, or documentation.

---

## ğŸ“„ License

MIT License. See [LICENSE](./LICENSE) for full terms.

---

## ğŸ™Œ Acknowledgements

Inspired by DORA metrics and DevOps Research & Assessment reports.  
Built for teams aiming to improve visibility and reduce delivery friction.

---

## ğŸ’¡ Optional Enhancements

Let me know if you'd like:

- A `requirements.txt` auto-generated from your environment  
- Badges (build status, license, Python version, etc.)  
- To convert this into a `docs/` site with Markdown pages or Sphinx


